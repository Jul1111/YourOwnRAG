# YourOwnRAG - Your Own Retrieval Augmented Generation Chatbot

Un chatbot aliment√© par **Retrieval Augmented Generation (RAG)** qui utilise une base de donn√©es vectorielle pour r√©pondre aux questions en se basant sur vos documents personnalis√©s.

## üéØ Fonctionnalit√©s

- **Chatbot Interactif** : Pose des questions et re√ßois des r√©ponses en temps r√©el
- **Base de Donn√©es Vectorielle** : Indexation intelligent des documents via embeddings
- **Recherche S√©mantique** : Retrouve les chunks les plus pertinents gr√¢ce √† la similarit√© cosinus
- **Support Multi-Format** : Texte brut (.txt), PDF, JSON, YAML, Code source (Python, JavaScript, Java, C++, etc.)
- **Splitting Intelligent** : D√©coupe automatiquement les fichiers selon leur type pour une meilleure coh√©rence
- **Mod√®les Open Source** : Utilise Ollama avec des mod√®les l√©gers et efficaces
- **Interface Conviviale** : S√©lection graphique de fichiers/dossiers (macOS/Windows)
- **Pr√©paration BDD** : Architecture pr√™te pour la persistance future en base de donn√©es

## üèóÔ∏è Architecture

```
YourOwnRAG/
‚îú‚îÄ‚îÄ main.py                          # Point d'entr√©e - CLI + s√©lection fichiers
‚îú‚îÄ‚îÄ requirements.txt                 # D√©pendances
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ loadingDataset.py           # Chargement & splitting multi-format
‚îÇ   ‚îú‚îÄ‚îÄ implementVectorDB.py         # Gestion base vectorielle avec embeddings
‚îÇ   ‚îî‚îÄ‚îÄ retrievalFunction.py         # Recherche s√©mantique (similarit√© cosinus)
‚îú‚îÄ‚îÄ tmp/
‚îÇ   ‚îú‚îÄ‚îÄ cat-facts.txt                # Donn√©es d'exemple
‚îÇ   ‚îî‚îÄ‚îÄ archive/                     # Autres donn√©es
‚îú‚îÄ‚îÄ data/                            # Votre dossier de donn√©es personnalis√©es
‚îî‚îÄ‚îÄ vector_db/                       # Stockage des embeddings (futur)
```

## üì¶ Installation

### 1. Cloner le repo

```bash
git clone https://github.com/Jul1111/YourOwnRAG.git
cd YourOwnRAG/Version1
```

### 2. Cr√©er un environnement virtuel (Python 3.9+)

```bash
python3.9 -m venv .venv
source .venv/bin/activate  # macOS/Linux
# ou
.venv\Scripts\activate  # Windows
```

### 3. Installer les d√©pendances

```bash
pip install -r requirements.txt
```

### 4. Configurer Ollama

Le projet utilise [Ollama](https://ollama.ai/) pour les embeddings et le mod√®le de langage.

```bash
# Installer Ollama depuis https://ollama.ai/
# Puis d√©marrer Ollama en arri√®re-plan
ollama serve

# Dans un autre terminal, t√©l√©charger les mod√®les n√©cessaires
ollama pull hf.co/CompendiumLabs/bge-base-en-v1.5-gguf  # Embeddings
ollama pull hf.co/bartowski/Llama-3.2-1B-Instruct-GGUF   # Mod√®le de langage
```

## üöÄ Utilisation

### Lancer le chatbot

```bash
python3 main.py
```

### Menu de Configuration

Au d√©marrage, vous avez 4 options :

```
Options:
1. S√©lectionner des fichiers      ‚Üí Ouvre un dialogue pour choisir des fichiers
2. S√©lectionner des dossiers      ‚Üí Ouvre un dialogue pour choisir des dossiers
3. Continuer sans charger         ‚Üí Utilise la BDD existante (futur)
4. Utiliser les sources par d√©faut ‚Üí Charge tmp/cat-facts.txt
```

Choisissez l'option 1 ou 2 pour une s√©lection graphique native (macOS/Windows).

### Exemple d'Interaction

```
Configuration de la base de connaissance
============================================================

Options:
1. S√©lectionner des fichiers
2. S√©lectionner des dossiers
3. Continuer sans charger (utiliser BDD existante)
4. Utiliser les sources par d√©faut

Choisissez une option (1-4): 1

[Dialogue de s√©lection de fichiers s'ouvre...]

‚úì 1 fichier(s) s√©lectionn√©(s)
  - document.pdf

Chargement des sources...
üìÑ Traitement du fichier: document.pdf
‚úì document.pdf (pdf) -> 156 chunks

‚úÖ R√©sum√©: 1 fichier(s) trait√©(s), 156 chunk(s) cr√©√©(s)

==================================================
Chatbot is ready! Type "exit" to quit.
==================================================

You: Quelle est l'histoire des clowders?

Retrieved knowledge:
 - (similarity: 0.92) Un clowder est un groupe de chats...
 - (similarity: 0.87) Les clowders sont souvent observ√©s...

Chatbot response: Un clowder est un groupe de chats qui vivent ensemble...

You: exit
Goodbye!
```

## üìö Formats Support√©s

### Fichiers de Code

- [ ] **Python** (.py)
- [ ] **JavaScript/TypeScript** (.js, .ts, .jsx, .tsx)
- [ ] **Java** (.java)
- [ ] **C/C++** (.c, .cpp, .h)
- [ ] **C#** (.cs)
- [ ] **Ruby** (.rb)
- [ ] **Go** (.go)
- [ ] **Rust** (.rs)
- [ ] **PHP** (.php)
- [ ] **Swift** (.swift)
- [ ] **SQL** (.sql)

### Fichiers de Configuration

- [ ] **JSON** (.json)
- [ ] **YAML** (.yaml, .yml)
- [ ] **TOML** (.toml)
- [ ] **INI** (.ini)
- [ ] **XML** (.xml)

### Documents

- [x] **Texte brut** (.txt) ‚úÖ Test√© et valid√©
- [ ] **Markdown** (.md)
- [ ] **PDF** (.pdf)

## üîß Fonctionnement Technique

### 1. **Chargement et Splitting Intelligent**

Le syst√®me d√©tecte automatiquement le type de fichier et applique la bonne strat√©gie de splitting :

- **Code** : D√©coupe par fonctions/classes (`def`, `class`, `\n\n`)
- **Configuration** : D√©coupe par lignes (`\n`, espaces)
- **Texte** : D√©coupe par paragraphes (`\n\n`, `\n`)
- **PDF** : D√©coupe par pages puis par paragraphes

Chaque chunk : **300-500 caract√®res** avec **chevauchement de 50 caract√®res**

### 2. **Cr√©ation de la Base Vectorielle**

```
Fichier ‚Üí D√©coupage en chunks ‚Üí Embedding via Ollama ‚Üí Base vectorielle
```

### 3. **Recherche S√©mantique**

```
Question ‚Üí Embedding ‚Üí Similarit√© cosinus ‚Üí Top 3 chunks les plus proches
```

### 4. **G√©n√©ration de R√©ponse**

```
Chunks pertinents + Question ‚Üí LLM Ollama ‚Üí R√©ponse contextualis√©e
```

## üõ†Ô∏è Configuration Avanc√©e

### Modifier les Param√®tres de Splitting

Dans [src/loadingDataset.py](src/loadingDataset.py) :

```python
SPLITTING_STRATEGIES = {
    'code': RecursiveCharacterTextSplitter(
        chunk_size=500,        # Augmentez pour plus de contexte
        chunk_overlap=50,      # Chevauchement entre chunks
        separators=[...],      # Ordre de priorit√© de s√©paration
    ),
    # ... autres strat√©gies
}
```

### Changer les Mod√®les

Dans [src/implementVectorDB.py](src/implementVectorDB.py) :

```python
EMBEDDING_MODEL = 'hf.co/CompendiumLabs/bge-base-en-v1.5-gguf'
LANGUAGE_MODEL = 'hf.co/bartowski/Llama-3.2-1B-Instruct-GGUF'
```

Remplacez par d'autres mod√®les disponibles sur Ollama.

### Ajuster la Recherche

Dans [main.py](main.py), modifiez `top_n` :

```python
retrieved_knowledge = retrieve(input_query, top_n=5)  # Au lieu de 3
```

## üìù Roadmap

- [x] Support multi-format (texte, code, PDF, JSON, YAML)
- [x] Splitting intelligent par type
- [x] Menu de s√©lection graphique
- [x] Chatbot interactif
- [ ] Persistance en base de donn√©es SQLite/PostgreSQL
- [ ] API REST (FastAPI)
- [ ] Interface web (Streamlit/Gradio)
- [ ] Fine-tuning sur domaines sp√©cifiques
- [ ] Support du multi-langue
- [ ] Cache des embeddings
- [ ] Historique des conversations

## üõ°Ô∏è Technologies Utilis√©es

- **Ollama** : Mod√®les LLM open source (embeddings + g√©n√©ration)
- **LangChain** : Splitting de texte et orchestration
- **PyPDF** : Extraction de texte depuis PDFs
- **Python 3.9+** : Langage principal

## üìÑ Licence

MIT License - Voir LICENSE pour plus de d√©tails

## ‚ú® Contributions

Les contributions sont bienvenues ! Vous pouvez :

- Signaler des bugs via les Issues
- Proposer des am√©liorations
- Soumettre des PRs

## üìû Support

Pour toute question ou probl√®me, ouvrez une issue sur le repo GitHub !

---

**Bon RAGing ! üöÄ**
