# YourOwnRAG - Your Own Retrieval Augmented Generation Chatbot

Un chatbot alimentÃ© par **Retrieval Augmented Generation (RAG)** qui utilise une base de donnÃ©es vectorielle pour rÃ©pondre aux questions en se basant sur vos documents personnalisÃ©s.

## ğŸ¯ FonctionnalitÃ©s

- **Chatbot Interactif** : Pose des questions et reÃ§ois des rÃ©ponses en temps rÃ©el
- **Base de DonnÃ©es Vectorielle** : Indexation intelligent des documents via embeddings
- **Recherche SÃ©mantique** : Retrouve les chunks les plus pertinents grÃ¢ce Ã  la similaritÃ© cosinus
- **Support Multi-Format** : Texte brut (.txt), PDF, JSON, YAML, Code source (Python, JavaScript, Java, C++, etc.)
- **Splitting Intelligent** : DÃ©coupe automatiquement les fichiers selon leur type pour une meilleure cohÃ©rence
- **ModÃ¨les Open Source** : Utilise Ollama avec des modÃ¨les lÃ©gers et efficaces
- **Interface Conviviale** : SÃ©lection graphique de fichiers/dossiers (macOS/Windows)
- **PrÃ©paration BDD** : Architecture prÃªte pour la persistance future en base de donnÃ©es

## ğŸ—ï¸ Architecture

```
YourOwnRAG/
â”œâ”€â”€ main.py                          # Point d'entrÃ©e - CLI + sÃ©lection fichiers
â”œâ”€â”€ requirements.txt                 # DÃ©pendances
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ loadingDataset.py           # Chargement & splitting multi-format
â”‚   â”œâ”€â”€ implementVectorDB.py         # Gestion base vectorielle avec embeddings
â”‚   â””â”€â”€ retrievalFunction.py         # Recherche sÃ©mantique (similaritÃ© cosinus)
â”œâ”€â”€ tmp/
â”‚   â”œâ”€â”€ cat-facts.txt                # DonnÃ©es d'exemple
â”‚   â””â”€â”€ archive/                     # Autres donnÃ©es
â”œâ”€â”€ data/                            # Votre dossier de donnÃ©es personnalisÃ©es
â””â”€â”€ vector_db/                       # Stockage des embeddings (futur)
```

## ğŸ“¦ Installation

### 1. Cloner le repo

```bash
git clone https://github.com/Jul1111/YourOwnRAG.git
cd YourOwnRAG/Version1
```

### 2. CrÃ©er un environnement virtuel (Python 3.9+)

```bash
python3.9 -m venv .venv
source .venv/bin/activate  # macOS/Linux
# ou
.venv\Scripts\activate  # Windows
```

### 3. Installer les dÃ©pendances

```bash
pip install -r requirements.txt
```

### 4. Configurer Ollama

Le projet utilise [Ollama](https://ollama.ai/) pour les embeddings et le modÃ¨le de langage.

```bash
# Installer Ollama depuis https://ollama.ai/
# Puis dÃ©marrer Ollama en arriÃ¨re-plan
ollama serve

# Dans un autre terminal, tÃ©lÃ©charger les modÃ¨les nÃ©cessaires
ollama pull hf.co/CompendiumLabs/bge-base-en-v1.5-gguf  # Embeddings
ollama pull hf.co/bartowski/Llama-3.2-1B-Instruct-GGUF   # ModÃ¨le de langage
```

## ğŸš€ Utilisation

### Lancer le chatbot

```bash
python3 main.py
```

### Menu de Configuration

Au dÃ©marrage, vous avez 4 options :

```
Options:
1. SÃ©lectionner des fichiers      â†’ Ouvre un dialogue pour choisir des fichiers
2. SÃ©lectionner des dossiers      â†’ Ouvre un dialogue pour choisir des dossiers
3. Continuer sans charger         â†’ Utilise la BDD existante (futur)
4. Utiliser les sources par dÃ©faut â†’ Charge tmp/cat-facts.txt
```

Choisissez l'option 1 ou 2 pour une sÃ©lection graphique native (macOS/Windows).

### Exemple d'Interaction

```
Configuration de la base de connaissance
==================================================
Chatbot is ready! Type "exit" to quit.
==================================================

You: Quelle est l'histoire des clowders?

Retrieved knowledge:
 - (similarity: 0.92) Un clowder est un groupe de chats...
 - (similarity: 0.87) Les clowders sont souvent observÃ©s...

Chatbot response: Un clowder est un groupe de chats qui vivent ensemble...

You: exit
Goodbye!
```

## ğŸ“š Formats SupportÃ©s

### Fichiers de Code

- [ ] **Python** (.py) (en cours)
- [ ] **JavaScript/TypeScript** (.js, .ts, .jsx, .tsx) (en cours)
- [ ] **Java** (.java) (en cours)
- [ ] **C/C++** (.c, .cpp, .h) (en cours)
- [ ] **C#** (.cs) (en cours)
- [ ] **Ruby** (.rb) (en cours)
- [ ] **Go** (.go) (en cours)
- [ ] **Rust** (.rs) (en cours)
- [ ] **PHP** (.php) (en cours)
- [ ] **Swift** (.swift) (en cours)
- [ ] **SQL** (.sql) (en cours)

### Fichiers de Configuration

- [ ] **JSON** (.json) (en cours)
- [ ] **YAML** (.yaml, .yml) (en cours)
- [ ] **TOML** (.toml) (en cours)
- [ ] **INI** (.ini) (en cours)
- [ ] **XML** (.xml) (en cours)

### Documents

- [x] **Texte brut** (.txt)
- [ ] **Markdown** (.md) (en cours)
- [ ] **PDF** (.pdf) (en cours)

## ğŸ”§ Fonctionnement Technique

### 1. **Chargement et Splitting Intelligent**

Le systÃ¨me dÃ©tecte automatiquement le type de fichier et applique la bonne stratÃ©gie de splitting :

- **Code** : DÃ©coupe par fonctions/classes (`def`, `class`, `\n\n`)
- **Configuration** : DÃ©coupe par lignes (`\n`, espaces)
- **Texte** : DÃ©coupe par paragraphes (`\n\n`, `\n`)
- **PDF** : DÃ©coupe par pages puis par paragraphes

Chaque chunk : **300-500 caractÃ¨res** avec **chevauchement de 50 caractÃ¨res**

### 2. **CrÃ©ation de la Base Vectorielle**

```
Fichier â†’ DÃ©coupage en chunks â†’ Embedding via Ollama â†’ Base vectorielle
```

### 3. **Recherche SÃ©mantique**

```
Question â†’ Embedding â†’ SimilaritÃ© cosinus â†’ Top 3 chunks les plus proches
```

### 4. **GÃ©nÃ©ration de RÃ©ponse**

```
Chunks pertinents + Question â†’ LLM Ollama â†’ RÃ©ponse contextualisÃ©e
```

## ğŸ› ï¸ Configuration AvancÃ©e

### Modifier les ParamÃ¨tres de Splitting

Dans [src/loadingDataset.py](src/loadingDataset.py) :

```python
SPLITTING_STRATEGIES = {
    'code': RecursiveCharacterTextSplitter(
        chunk_size=500,        # Augmentez pour plus de contexte
        chunk_overlap=50,      # Chevauchement entre chunks
        separators=[...],      # Ordre de prioritÃ© de sÃ©paration
    ),
    # ... autres stratÃ©gies
}
```

### Changer les ModÃ¨les

Dans [src/implementVectorDB.py](src/implementVectorDB.py) :

```python
EMBEDDING_MODEL = 'hf.co/CompendiumLabs/bge-base-en-v1.5-gguf'
LANGUAGE_MODEL = 'hf.co/bartowski/Llama-3.2-1B-Instruct-GGUF'
```

Remplacez par d'autres modÃ¨les disponibles sur Ollama.

### Ajuster la Recherche

Dans [main.py](main.py), modifiez `top_n` :

```python
retrieved_knowledge = retrieve(input_query, top_n=5)  # Au lieu de 3
```

## ğŸ“ Roadmap

- [x] Support multi-format (texte, code, PDF, JSON, YAML)
- [x] Splitting intelligent par type
- [x] Menu de sÃ©lection graphique
- [x] Chatbot interactif
- [ ] Persistance en base de donnÃ©es SQLite/PostgreSQL
- [ ] API REST (FastAPI)
- [ ] Interface web (Streamlit/Gradio)
- [ ] Fine-tuning sur domaines spÃ©cifiques
- [ ] Support du multi-langue
- [ ] Cache des embeddings
- [ ] Historique des conversations

## ğŸ›¡ï¸ Technologies UtilisÃ©es

- **Ollama** : ModÃ¨les LLM open source (embeddings + gÃ©nÃ©ration)
- **LangChain** : Splitting de texte et orchestration
- **PyPDF** : Extraction de texte depuis PDFs
- **Python 3.9+** : Langage principal

## ğŸ“„ Licence

MIT License - Voir LICENSE pour plus de dÃ©tails

## âœ¨ Contributions

Les contributions sont bienvenues ! Vous pouvez :

- Signaler des bugs via les Issues
- Proposer des amÃ©liorations
- Soumettre des PRs

## ğŸ“ Support

Pour toute question ou problÃ¨me, ouvrez une issue sur le repo GitHub !

---

**Bon RAGing ! ğŸš€**
